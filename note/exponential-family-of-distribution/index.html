<!DOCTYPE html>
<html lang="en-gb">
    <head>
        <meta charset="UTF-8">
        <meta name="apple-mobile-web-app-capable" content="yes">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="Static site using Zola">
        <meta name="author" content="gali-leilei">
        <title></title>
        <link rel="stylesheet" href="https://gali-leilei.github.io/site.css"/>
        
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async 
  src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js">
</script>

    </head>
    <body>

        <header>
            <nav class="container">
                <a class="white" href="https://gali-leilei.github.io/about/" class="nav-link">About</a>
                <a class="white" href="https://gali-leilei.github.io/resume/" class="nav-link">Resume</a>
                <a class="white" href="https://gali-leilei.github.io/note/" class="nav-link">Notes</a>
                <a class="white" href="https://gali-leilei.github.io/projects/" class="nav-link">Projects</a>
            </nav>
        </header>

        <div class="content  content--reversed">
            
<div class="documentation__content">
<h1 class="title">
  What is Exponential Family of Distribution
</h1>
<p class="subtitle"><strong>2021-05-13</strong></p>
  <h2 id="motivation">Motivation</h2>
<p>To understand the first 4 page of <a href="https://repository.rothamsted.ac.uk/download/25425465aa52d05e1a9e553b2daddeeffe15d0ba40f5f9b8937aaab5c3d29e1d/4410096/Nelder%201972.pdf">Generalized Linear Models</a> by Nelder and Wedderburn, without a formal background in Statistics. That took me a while.</p>
<p><a href="https://www.youtube.com/watch?v=X-ix97pw0xY">This 4 hours lecture series</a> from MIT helped a lot. Highly recommend.</p>
<p>By the end of the post, I hope I have answered this question:</p>
<ul>
<li>what is the <strong>intuition</strong> of exponential family of distribution?</li>
</ul>
<h2 id="sufficient-statistics">Sufficient Statistics</h2>
<p><strong>Definition</strong> A <em>statistics</em> is a function $T := r(X_1, X_2, \ldots, X_n)$ of the random samples $X_1, X_2, \ldots, X_n \in S$. For now, suppose the samples are sampled from a unknown distribution $\pi$.</p>
<p><strong>Definition</strong> A <em>statistical model</em> is a pair of $(S, P)$, where $S$ is the set of all possible observation and $P$ is the set of probability distribution on $S$. Note: S for Sample space and P for Prior distributions.</p>
<p><strong>Definition</strong> Given <em>statistical model</em>, $(S, P)$, suppose that $P$ is parameterized: $P := {P_\theta \mid \theta \in O}$. Then the model is:</p>
<ul>
<li><em>parametric</em> if $O$ is finite dimensional,</li>
<li><em>non-parametric</em> if $O$ is infinite dimensional,</li>
<li><em>semi-parametric</em> if $O$ has both finite and infinite dimensional parameters.</li>
</ul>
<p>That was a mouthful so that I can say the following:</p>
<p><strong>Definition</strong> Let $(S, P_\theta)$ be a <em>parametric statistical models</em> and $X_1, X_2, \ldots, X_n \in S$ be i.i.d. samples generated from $P_\theta^<em>$ for some unknown $\theta^</em> \in O$. An <em>estimator</em> for $\theta^*$, $T(X_1, X_2, \ldots, X_n)$, is a <em>statistics</em> that maps into $O$, i.e. $T: S^n \rightarrow O$.</p>
<p>In general, the true distribution, $\pi$, that generates the samples may not be in the prior $P_\theta$. That should be fine if the set $P_\theta$ is dense enough: for any $\epsilon &gt; 0$, there exists $\theta$ such that $d(P_\theta, \pi) &lt; \epsilon$.</p>
<p><strong>Definition</strong> Let $(S, P_\theta)$ be a <em>parametric statistical models</em> and $X_1, X_2, \ldots, X_n \in S$ be i.i.d. samples generated from $P_\theta^<em>$ for some unknown $\theta^</em> \in O$. A <em>statistics</em>, $T(x)$, is <strong>sufficient</strong> if</p>
<ul>
<li>no other statistics, $h: S^n \rightarrow O$, provide additional information on $\theta^*$, or</li>
<li>(Fisher-Neyman factorization) there exists non-negative functions $f: S^n \rightarrow R, g: S^n \rightarrow R$ such that $P_\theta(x) = h(x) g(T(x); \theta)$</li>
</ul>
<p>Sufficient statistics formalizes <a href="https://www.math.arizona.edu/~tgk/466/sufficient.pdf">the idea</a> that, two sets of data yielding the same value for statistics $T(x)$, would yield the same inference about $\theta$. The notion of sufficiency makes most sense in term of information entropy; there is no loss of information regarding $\theta$ when compressing from samples, $X_1, X_2, \ldots, X_n$, to statistics, $T(X_1, X_2, \ldots, X_n)$.</p>
<p>If we were to describe the distribution, a parametric model (distribution as a function of the statistics) is as good as a non-parametric model (distribution as a function of the data points), <strong>if and only if</strong> the distribution admits sufficient statistics.</p>
<p>In layman's term, no information is lost when <strong>compressing</strong> arbitrary n data points (samples) into a fixed number of parameters (statistics).</p>
<p>So what does it have to do with exponential family of distributions?</p>
<p>TODO: use factorization theorem instead, drop the assumption on i.i.d.</p>
<h2 id="essence-of-exponential-family-of-distribution">Essence of Exponential Family of Distribution</h2>
<p><a href="http://www.ams.org/journals/tran/1936-039-03/S0002-9947-1936-1501854-3/S0002-9947-1936-1501854-3.pdf">It turns out that</a>, a <em>sufficient</em> condition (not a pun) for distributions admitting a sufficient statistics is that its distribution obeys a certain form:</p>
<p><strong>Definition</strong> (slide 11 from [MIT course notes][glm-mit-notes]) <em>exponential family of distribution</em> is a parametric distribution where:</p>
<p>A family of distribution ${P_\theta: \theta \in \Omega }$, $\Omega \subset \mathbb{R}^k$ is said to be a
$k$ -parameter exponential family on $\mathbb{R}^q$ , if there exist real valued
functions:</p>
<ul>
<li>$\mu_1, \mu_2, \ldots, \mu_k$ and $B$ of $\theta$,</li>
<li>$T_1, T_2, \ldots, T_k$ and $h$ of $x \sub \mathbb{R}^q$ such that the density
function (pmf or pdf) of $P_\theta$ can be written as</li>
</ul>
<p>$$
p_\theta(x) := exp \left( \sum_{i = 1}^k \mu_i(\theta) T_i(x) - B(\theta)\right) h(x)
$$</p>
<p>For most practical purposes, $k$ is rarely more than 1. See this chart of commonly used distributions (FIXME: insert taxonomy of various distributions)</p>
<h2 id="definition-of-canonical-exponential-family-of-distribution">Definition of Canonical Exponential Family of Distribution</h2>
<p><strong>Definition</strong> A exponential family of distribution for $k = 1, x \in \mathbb{R}$ is canonical if</p>
<ul>
<li>the parameter $\theta \in \mathbb{R}$ only interacts <strong>linearly</strong> with observation/statistics, or equivalently</li>
<li>it can be written as $p(x; \theta) = exp(\langle t(x), \theta \rangle) a(x) b(\theta)$, or equivalently</li>
<li>it has the following canonical decomposition</li>
</ul>
<p>$$
p(x; \theta) := exp( \langle x, \theta \rangle - F(\theta) + k(x))
$$</p>
<p>where</p>
<ul>
<li>$t(x) := x$ is the sufficient statistics being identity function,</li>
<li>$\theta$ is the natural parameters,</li>
<li>$F(\cdot)$ is the log-normalizer,</li>
<li>$k(x)$ is the carrier measure.</li>
</ul>
<h3 id="properties-of-canonical-exponential-family-of-distribution">Properties of Canonical Exponential Family of Distribution</h3>
<p>The exponential form of pdf allows us to derive mean and variance analytically, which will be useful in discussing Generalized Linear Model.</p>
<p><strong>Lemma</strong> Let $p(x; \theta)$ be a probability density function with parameter $\theta$ and $l(\theta) := log p(x; \theta)$ be the likelihood function. We have</p>
<ul>
<li>$E[\frac{\partial l}{\partial \theta}] = 0$ because</li>
</ul>
<p>$$
E[\frac{\partial l}{\partial \theta}] := \int \frac{p'(x; \theta)}{p(x; \theta)} p(x; \theta) dx = \int \frac{d}{d \theta} \left( p(x; \theta) \right) dx= \frac{d}{d \theta} \int p(x; \theta) dx = 0
$$</p>
<ul>
<li>$E[\frac{\partial l^2}{\partial^2 \theta}] = - E[(\frac{\partial l}{\partial \theta})^2]$ because</li>
</ul>
<p>$$
E\left[\frac{\partial l^2}{\partial^2 \theta}\right] := E\left[\frac{d}{d \theta}\left(\frac{p'(x; \theta)}{p(x; \theta)}\right)\right] = \int \frac{p''(x; \theta)p(x; \theta) - p'(x; \theta)^2}{p^2(x; \theta)} p(x; \theta)dx = \int p''(x; \theta) dx - \int \left(\frac{p'(x; \theta)}{p(x; \theta)}\right)^2 p(x; \theta) dx = - E\left[\left(\frac{\partial l}{\partial \theta}\right)^2\right]
$$</p>
<p><strong>Claim</strong> Suppose a distribution $X \sim p(x ; \theta) := exp (x \theta - F(\theta) + k(x))$, then</p>
<ul>
<li>$E[X | \theta] := F'(\theta)$ and</li>
<li>$Var[X | \theta] := F''(\theta)$.</li>
</ul>
<p>This follows from applying the lemma above.</p>

</div>

        </div>
        <!-- <footer>
            ©2017-2025 — <a class="white" href="https://vincentprouillet.com">Vincent Prouillet</a> and <a class="white" href="https://github.com/getzola/zola/graphs/contributors">contributors</a>
        </footer> -->

    </body>
</html>
